{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97258,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T00:12:16.120462Z","iopub.execute_input":"2025-04-20T00:12:16.120730Z","iopub.status.idle":"2025-04-20T00:12:17.420379Z","shell.execute_reply.started":"2025-04-20T00:12:16.120705Z","shell.execute_reply":"2025-04-20T00:12:17.419295Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Claim Verification with RAG and Gemini\n\n## Problem Breakdown\n**Goal:**\nThe goal of this project is to **fact-check a user-provided claim** by:\n\n* Searching the web for relevant evidence,\n* Use embeddings + vector search (**RAG**) to filter the most relevant evidence,\n* Passing that evidence to a large language model (**Gemini 1.5 Flash**),\n* Returning a structured **verdict** (true, false, or uncertain) along with an **explanation**, **supporting evidence**, and **source URLs**,\n* Providing an intuitive **Gradio UI** to interact with the system.\n","metadata":{}},{"cell_type":"markdown","source":"## Tools & Infrastructure \n\n\n**Core AI Component:**\n* Perform real-time Google searches through a programmable API (`langchain_google_community.GoogleSearchAPIWrapper`)\n* Convert text documents into vector embeddings for similarity search (`langchain_huggingface.HuggingFaceEmbeddings`)\n* Wrap raw search results as Document objects with metadata (`langchain.docstore.document.Document`)\n* Store and retrieve documents using vector similarity (RAG retrieval) (`langchain.vectorstores.FAISS`)\n* Run Gemini LLM to generate explanations and fact-check claims (`google.generativeai.GenerativeModel`)\n\n**Deployment:**\n* Build a web UI for interactive claim testing (`Gradio`)","metadata":{}},{"cell_type":"markdown","source":"# Code","metadata":{}},{"cell_type":"markdown","source":"## Install Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install -q langchain sentence-transformers faiss-cpu langchain_huggingface gradio langchain_google_community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T00:12:17.421389Z","iopub.execute_input":"2025-04-20T00:12:17.421957Z","iopub.status.idle":"2025-04-20T00:13:00.978276Z","shell.execute_reply.started":"2025-04-20T00:12:17.421920Z","shell.execute_reply":"2025-04-20T00:13:00.976938Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.6/99.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m433.9/433.9 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.24.2 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Setup & Configuration\n1. Get Google API Key\n(âš ï¸ **Make sure the key is unrestricted or allow it to access Custom Search API publicly**)\n\n2. Get Custom Search Engine (CSE) ID\n\n3. Add Google API Key and CSE as *secrets* in Kaggle: Go to Add-ons > Secrets > Add Secret and add:\n","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nGOOGLE_CSE_ID = UserSecretsClient().get_secret(\"GOOGLE_CSE_ID\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T00:13:00.979516Z","iopub.execute_input":"2025-04-20T00:13:00.979802Z","iopub.status.idle":"2025-04-20T00:13:01.375861Z","shell.execute_reply.started":"2025-04-20T00:13:00.979778Z","shell.execute_reply":"2025-04-20T00:13:01.374961Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Model Configuration\nThe first block initializes the genai.Client from the google package using the provided API key (`GOOGLE_API_KEY`). It then lists all available models that support the generateContent action, printing the names of those models.\n\nThe second block configures a specific generative model, `gemini-1.5-pro`, using the `google.generativeai` package. The model is set up by calling `configure` with the API key, enabling access to the selected generative model for subsequent content generation tasks.","metadata":{}},{"cell_type":"code","source":"# ------- OPTIONAL ------\n# from google import genai\n# client = genai.Client(api_key=GOOGLE_API_KEY)\n\n# for m in client.models.list():\n#     if \"generateContent\" in m.supported_actions:\n#         print(m.name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google.generativeai import configure, GenerativeModel\n\nconfigure(api_key=GOOGLE_API_KEY)\nmodel = GenerativeModel(\"gemini-1.5-pro\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T00:14:06.404114Z","iopub.execute_input":"2025-04-20T00:14:06.404683Z","iopub.status.idle":"2025-04-20T00:14:08.457566Z","shell.execute_reply.started":"2025-04-20T00:14:06.404638Z","shell.execute_reply":"2025-04-20T00:14:08.456703Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Search Configuration\nInitialize the `GoogleSearchAPIWrapper` from the `langchain_google_community` package, which allows integration with Google Custom Search. It uses the provided Google API key (`GOOGLE_API_KEY`) and Custom Search Engine ID (`GOOGLE_CSE_ID`) to perform searches and retrieve relevant results for the given queries.","metadata":{}},{"cell_type":"code","source":"from langchain_google_community import GoogleSearchAPIWrapper\n\nsearch = GoogleSearchAPIWrapper(google_api_key=GOOGLE_API_KEY, google_cse_id=GOOGLE_CSE_ID)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T00:14:08.458680Z","iopub.execute_input":"2025-04-20T00:14:08.459103Z","iopub.status.idle":"2025-04-20T00:14:10.003092Z","shell.execute_reply.started":"2025-04-20T00:14:08.459081Z","shell.execute_reply":"2025-04-20T00:14:10.002032Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Embedding Model Configuration \nUse the `HuggingFaceEmbeddings` class from `langchain_huggingface` to load a pre-trained embedding model. Set the `model_name` to `\"sentence-transformers/all-MiniLM-L6-v2\"` for efficient and high-quality sentence embeddings, which will be used to convert text into vector representations for retrieval and similarity comparison.","metadata":{}},{"cell_type":"code","source":"from langchain_huggingface import HuggingFaceEmbeddings\n\nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T00:14:11.401771Z","iopub.execute_input":"2025-04-20T00:14:11.402234Z","iopub.status.idle":"2025-04-20T00:14:45.650010Z","shell.execute_reply.started":"2025-04-20T00:14:11.402199Z","shell.execute_reply":"2025-04-20T00:14:45.649080Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b711ffe71cdd4e8b92cff19723b3b065"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bcc2f02bd27446aa9e2db70ccc040d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d59983372b3d4707b91994f89ce058ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee97b08f636e42a3b776bb321eff70a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66baab0e42fb42859113e89549316f16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4181e174ce944bb8fd145c8d76f2ace"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d15803fc01a44c27b6d9d62c72cb22d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95758b6822f447cbbe0a969584efa175"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c9c3e7ceb5f4266801b1b0dbc816314"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c522ffa664c44bc93962ac62d61c1ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e80e1c254eb4fea90d933b87a723a1b"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"### Prompt Instruction\nDefine a fact-checking prompt for the language model to verify a claim using retrieved online evidence. The prompt asks the model to return a structured JSON response including:\n\n* A clear **verdict**: \"true\", \"false\", or \"uncertain\"\n* A short **explanation** supported by evidence\n* A list of **key quotes** from the evidence\n* A list of **source URLs** the quotes came from\n\nThis ensures transparency and traceability in the fact-checking result.","metadata":{}},{"cell_type":"code","source":"ANALYSIS_PROMPT = \"\"\"\nYou are a fact-checking expert. Your task is to verify the truth of a claim based on online evidence.\n\nClaim:\n\"{claim}\"\n\nEvidence (retrieved from web search):\n{evidence_formatted}\n\nRespond in JSON format with:\n- verdict: \"true\", \"false\", or \"uncertain\"\n- explanation: a concise explanation justifying the verdict using quotes from evidence\n- evidence_used: list of key quotes that support your verdict\n- sources: list of URLs the quotes came from\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T00:14:48.918883Z","iopub.execute_input":"2025-04-20T00:14:48.919653Z","iopub.status.idle":"2025-04-20T00:14:48.924669Z","shell.execute_reply.started":"2025-04-20T00:14:48.919620Z","shell.execute_reply":"2025-04-20T00:14:48.923322Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Vector Store Construction\nUse `LangChain`'s `Document` and `FAISS` to build a vector store from Google search results:\n\n* Wrap each result's **snippet** and **link** into a Document, storing the **link** in the metadata.\n* Collect all documents and convert them into embeddings using the selected embedding model.\n* Store the embeddings in a FAISS vectorstore, allowing efficient similarity search for relevant evidence later.","metadata":{}},{"cell_type":"code","source":"from langchain.docstore.document import Document\nfrom langchain.vectorstores import FAISS\n\ndef build_vector_store(claim, search_results):\n    documents = []\n\n    for result in search_results:\n        text = f\"{result['snippet']} (Source: {result['link']})\"\n        doc = Document(page_content=text, metadata={\"source\": result['link']})\n        documents.append(doc)\n\n    vectorstore = FAISS.from_documents(documents, embedding_model)\n    return vectorstore","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T00:14:50.821854Z","iopub.execute_input":"2025-04-20T00:14:50.822213Z","iopub.status.idle":"2025-04-20T00:14:50.836108Z","shell.execute_reply.started":"2025-04-20T00:14:50.822186Z","shell.execute_reply":"2025-04-20T00:14:50.835289Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import time\nimport json\n\ndef verify_claim(claim: str):\n    print(f\"ğŸ§ Testing claim: '{claim}'\")\n    start_time = time.time()\n\n    try:\n        print(f\"[{time.time() - start_time:.1f}s] Starting search...\")\n        search_results_raw = search.results(claim, num_results=10)  # raw dicts\n        print(f\"âœ… Search completed ({len(search_results_raw)} results, {time.time() - start_time:.1f}s)\")\n    except Exception as e:\n        print(f\"âŒ Search failed: {type(e).__name__}: {str(e)}\")\n        return None\n\n    try:\n        print(f\"[{time.time() - start_time:.1f}s] Building vector store...\")\n        vectorstore = build_vector_store(claim, search_results_raw)\n\n        print(f\"[{time.time() - start_time:.1f}s] Retrieving relevant chunks...\")\n        docs = vectorstore.similarity_search(claim, k=5)\n\n        # Format evidence\n        evidence_formatted = \"\\n\\n\".join(\n            [f\"- \\\"{doc.page_content}\\\"\" for doc in docs]\n        )\n\n        prompt = ANALYSIS_PROMPT.format(claim=claim, evidence_formatted=evidence_formatted)\n\n        print(f\"[{time.time() - start_time:.1f}s] Calling LLM...\")\n        response = model.generate_content(contents=[{\"parts\": [{\"text\": prompt}]}])\n        print(f\"âœ… LLM completed ({time.time() - start_time:.1f}s)\")\n\n        cleaned = response.text.strip().strip(\"```json\").strip(\"```\")\n        return json.loads(cleaned)\n    except Exception as e:\n        print(f\"âŒ LLM failed: {type(e).__name__}: {str(e)}\")\n        return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T00:38:41.136540Z","iopub.execute_input":"2025-04-20T00:38:41.136930Z","iopub.status.idle":"2025-04-20T00:38:41.146743Z","shell.execute_reply.started":"2025-04-20T00:38:41.136900Z","shell.execute_reply":"2025-04-20T00:38:41.145585Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"result = verify_claim(\"Landing on moon was fake\")\nprint(\"ğŸ“ Result:\", json.dumps(result, indent=2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T00:33:18.900901Z","iopub.execute_input":"2025-04-20T00:33:18.901357Z","iopub.status.idle":"2025-04-20T00:33:25.951500Z","shell.execute_reply.started":"2025-04-20T00:33:18.901318Z","shell.execute_reply":"2025-04-20T00:33:25.949996Z"}},"outputs":[{"name":"stdout","text":"ğŸ§ Testing claim: 'Landing on moon was fake'\n[0.0s] Starting search...\nâœ… Search completed (10 results, 0.3s)\n[0.3s] Building vector store...\n[0.6s] Retrieving relevant chunks...\n[0.6s] Calling LLM...\nâœ… LLM completed (7.0s)\nğŸ“ Result: {\n  \"verdict\": \"false\",\n  \"explanation\": \"Multiple reliable sources debunk the claim that the moon landing was faked. The provided evidence includes articles that address and refute common conspiracy theories, such as concerns about the Van Allen belts, the lack of stars in photographs, and the footprint/boot discrepancy.  While some sources present the arguments of conspiracy theorists, they do so in the context of debunking them. For example, the IOP article explains how the Apollo missions navigated the Van Allen belts.  Furthermore, the Reddit comment is not a credible source.\",\n  \"evidence_used\": [\n    \"\\\"Perhaps the most convincing argument that the landings were faked has to do with something called the Van Allen belts...\\\" (This source goes on to explain how the Van Allen belts were navigated, debunking the conspiracy)\",\n    \"\\\"Conspiracy theories that the moon landing was actually a hoax... began...\\\" (This source discusses the history and debunking of moon landing conspiracy theories)\",\n    \"\\\"Conspiracy theorists argue that the lack of stars in the Apollo 11 mission photographs prove that the event was staged. ...\\\" (This source proceeds to debunk the 'missing stars' theory)\",\n    \"\\\"Conspiracy: footprint doesn't match boot. Conspiracy theorists claim that the Moon landing was faked ...\\\" (This source then explains why the footprint and boot seemingly don't match)\"\n  ],\n  \"sources\": [\n    \"https://www.iop.org/explore-physics/moon/how-do-we-know-we-went-to-the-moon\",\n    \"https://www.history.com/articles/moon-landing-fake-conspiracy-theories\",\n    \"https://www.rmg.co.uk/stories/topics/moon-landing-conspiracy-theories-debunked\",\n    \"https://www.bbc.co.uk/bitesize/articles/z3x6b7h\"\n  ]\n}\nğŸ§ Testing claim: '5G causes COVID-19'\n[0.0s] Starting search...\nâœ… Search completed (10 results, 0.2s)\n[0.2s] Building vector store...\n[0.5s] Retrieving relevant chunks...\n[0.5s] Calling LLM...\nâœ… LLM completed (4.9s)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import gradio as gr\n\ndef explain_claim(claim):\n    result = verify_claim(claim)\n    if result is None:\n        return \"âš ï¸ Error: Could not generate a response. Please try again.\"\n    \n    verdict = result.get(\"verdict\", \"unknown\")\n    explanation = result.get(\"explanation\", \"\")\n    evidence = result.get(\"evidence_used\", [])\n    sources = result.get(\"sources\", [])\n    \n    response = f\"### âœ… Verdict: `{verdict}`\\n\\n\"\n    response += f\"**Explanation:** {explanation}\\n\\n\"\n    \n    if evidence:\n        response += \"**Evidence:**\\n\"\n        for item in evidence:\n            response += f\"- {item}\\n\"\n    \n    if sources:\n        response += \"\\n**Sources:**\\n\"\n        for src in sources:\n            response += f\"- {src}\\n\"\n    \n    return response\n\n# Launch Gradio UI\ngr.Interface(\n    fn=explain_claim,\n    inputs=gr.Textbox(lines=2, label=\"Enter a Claim\"),\n    outputs=gr.Markdown(label=\"Analysis\"),\n    title=\"Claim Verifier with RAG and Gemini ğŸ”\",\n    description=\"Enter a claim and get a verdict based on search evidence.\",\n    examples=[\"5G causes COVID-19\", \"The moon landing was faked\"]\n).launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T00:38:43.998764Z","iopub.execute_input":"2025-04-20T00:38:43.999092Z","iopub.status.idle":"2025-04-20T00:38:46.379142Z","shell.execute_reply.started":"2025-04-20T00:38:43.999066Z","shell.execute_reply":"2025-04-20T00:38:46.377670Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7871\nIt looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://2acf38086d292e9ad9.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://2acf38086d292e9ad9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stdout","text":"ğŸ§ Testing claim: '5G causes COVID-19'\n[0.0s] Starting search...\nâœ… Search completed (10 results, 0.2s)\n[0.2s] Building vector store...\n[0.5s] Retrieving relevant chunks...\n[0.5s] Calling LLM...\nâœ… LLM completed (4.7s)\nğŸ§ Testing claim: 'Can AI models like ChatGPT experience emotions?'\n[0.0s] Starting search...\nâœ… Search completed (10 results, 0.4s)\n[0.4s] Building vector store...\n[0.6s] Retrieving relevant chunks...\n[0.6s] Calling LLM...\nâœ… LLM completed (5.4s)\nğŸ§ Testing claim: '5G causes COVID-19'\n[0.0s] Starting search...\nâœ… Search completed (10 results, 0.4s)\n[0.4s] Building vector store...\n[0.7s] Retrieving relevant chunks...\n[0.7s] Calling LLM...\nâœ… LLM completed (4.9s)\nğŸ§ Testing claim: 'Can AI models like ChatGPT experience emotions?'\n[0.0s] Starting search...\nâœ… Search completed (10 results, 0.2s)\n[0.2s] Building vector store...\n[0.4s] Retrieving relevant chunks...\n[0.4s] Calling LLM...\nâœ… LLM completed (5.0s)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}